import os
import json
import subprocess
import sys
import urllib.parse # Added for URL encoding

from llama_index.core.tools import FunctionTool
from llama_index.readers.web import SimpleWebPageReader
from llama_index.readers.semanticscholar import SemanticScholarReader
from llama_index.tools.wikipedia import WikipediaToolSpec
from llama_index.tools.tavily_research import TavilyToolSpec
from llama_index.tools.duckduckgo import DuckDuckGoSearchToolSpec
from llama_index.core import VectorStoreIndex, StorageContext, load_index_from_storage
from llama_index.core import Settings 
from huggingface_hub import HfFileSystem

# --- Hugging Face RAG Configuration ---
from config import (
    HF_DATASET_ID, 
    HF_VECTOR_STORE_SUBDIR,
    PROJECT_ROOT, # Added for path mapping
    SOURCE_DATA_DIR, # Added for path mapping
    WEB_MARKDOWN_PATH, # Added for path mapping
    ADDITIONAL_SOURCE_DATA_DIR, # Added for path mapping
    HF_ADDITIONAL_SOURCE_DATA_UPLOAD_PATH # Added for path mapping
)

# Determine project root based on the script's location
# PROJECT_ROOT is now imported from config.py, so this line is redundant if config.py is at root
# PROJECT_ROOT = os.path.abspath(os.path.dirname(__file__))

# Ensure API keys are set as environment variables
# os.environ["TAVILY_API_KEY"] = "YOUR_TAVILY_API_KEY"
# os.environ["GOOGLE_API_KEY"] = "YOUR_GOOGLE_API_KEY"

# Directory where files generated by code interpreter should be accessible by the UI
UI_ACCESSIBLE_WORKSPACE_RELATIVE = "code_interpreter_ws"
UI_ACCESSIBLE_WORKSPACE = os.path.join(PROJECT_ROOT, UI_ACCESSIBLE_WORKSPACE_RELATIVE)
os.makedirs(UI_ACCESSIBLE_WORKSPACE, exist_ok=True)


# --- Individual Tool Initializations ---
def safe_code_interpreter(code: str) -> str:
    """
    Executes Python code and returns stdout/stderr.
    The code is executed in a way that it can save files to a pre-defined workspace.
    If the code saves a file, it should print '---DOWNLOAD_FILE---filename.ext' to stdout.
    """
    global UI_ACCESSIBLE_WORKSPACE # Ensure we're using the globally defined workspace
    try:
        # The code generated by the LLM should handle saving files to UI_ACCESSIBLE_WORKSPACE.
        # We provide the workspace path to the execution environment.
        # The LLM will be instructed to use this path.
        
        # For security and simplicity, we're directly executing the code string.
        # Ensure the LLM is prompted to write safe code.
        # The `subprocess` approach can be more isolating.
        # Here, we'll rely on the LLM being instructed to save files to the correct workspace.
        result = subprocess.run(
            [sys.executable, "-c", code],
            capture_output=True,
            text=True,
            check=False, # Don't raise an exception for non-zero exit codes
            timeout=60,  # Increased timeout for potentially longer operations
            cwd=UI_ACCESSIBLE_WORKSPACE # Set current working directory for the subprocess
        )
        # Check if there was an error during execution
        if result.stderr:
            # If there's an error, format the output to clearly indicate this
            return f"Python Code Execution Error:\n\nExecuted Code:\n```python\n{code}\n```\n\nError Output (StdErr):\n{result.stderr}\n\nStandard Output (StdOut):\n{result.stdout}"
        else:
            # Successful execution
            return f"Executed Code:\n```python\n{code}\n```\n\nStdOut:\n{result.stdout}\nStdErr:\n{result.stderr}" # stderr will be empty here
    except subprocess.TimeoutExpired:
        return "Error: Code execution timed out after 60 seconds."
    except FileNotFoundError:
        return "Error: Python interpreter not found. Cannot execute code."
    except Exception as e:
        return f"Error during code execution: {str(e)}"

def get_duckduckgo_tool(max_results: int = 5):
    """Initializes the DuckDuckGo search tool."""
    try:
        # Removed max_results from constructor, it's applied during query
        return DuckDuckGoSearchToolSpec().to_tool_list()[0]
    except NameError:
        print("Error: DuckDuckGoSearchToolSpec not found. Trying DuckDuckGoToolSpec.")
        try:
            from llama_index.tools.duckduckgo import DuckDuckGoToolSpec
            # Removed max_results from constructor, it's applied during query
            return DuckDuckGoToolSpec().to_tool_list()[0]
        except Exception as e_fallback:
            print(f"Error initializing DuckDuckGo Tool (fallback failed): {e_fallback}")
            return None
    except Exception as e:
        print(f"Error initializing DuckDuckGo Tool: {e}")
        return None

def get_tavily_tool(max_results: int = 5):
    """Initializes the Tavily search tool."""
    tavily_api_key = os.getenv("TAVILY_API_KEY")
    if not tavily_api_key:
        print("Warning: TAVILY_API_KEY not found in environment variables.")
        return None
    try:
        # Removed max_results from constructor, it's applied during query
        return TavilyToolSpec(api_key=tavily_api_key).to_tool_list()[0]
    except Exception as e:
        print(f"Error initializing Tavily Tool: {e}")
        return None

def get_wikipedia_tool():
    """Initializes the Wikipedia tool."""
    try:
        # WikipediaToolSpec does not have a direct max_results parameter in the constructor
        # The limit is usually applied during the query execution.
        # We will not modify this tool for the search results count setting.
        return WikipediaToolSpec().to_tool_list()[0]
    except Exception as e:
        print(f"Error initializing Wikipedia Tool: {e}")
        return None

def get_semantic_scholar_tool_for_agent(max_results: int = 5):
    """
    Initializes the Semantic Scholar tool.
    Returns a list containing the tool, suitable for an agent.
    """
    try:
        ss_reader = SemanticScholarReader()
        tool = FunctionTool.from_defaults(
            fn=lambda query: ss_reader.load_data(query=query, limit=max_results), # Use max_results as limit
            name="semantic_scholar_search",
            description="Searches Semantic Scholar for academic papers based on a query.",
        )
        return tool
    except Exception as e:
        print(f"Error initializing Semantic Scholar Tool: {e}")
        return None

def get_web_scraper_tool_for_agent():
    """
    Initializes the Web Scraper tool.
    Returns a list containing the tool, suitable for an agent.
    """
    try:
        loader = SimpleWebPageReader(html_to_text=True)
        tool = FunctionTool.from_defaults(
            fn=lambda url: loader.load_data(urls=[url]),
            name="web_scraper",
            description="Scrapes textual content from a given URL. This can be an HTML webpage or a direct link to a PDF document. Expects a single URL as input.",
        )
        return tool
    except Exception as e:
        print(f"Error initializing Web Scraper Tool: {e}")
        return None

def get_rag_tool_for_agent():
    """Initializes the RAG query tool by loading the SimpleVectorStore from Hugging Face Hub."""

    print(f"Attempting to load RAG index from Hugging Face Hub: datasets/{HF_DATASET_ID}/{HF_VECTOR_STORE_SUBDIR}")

    try:
        hf_token = os.getenv("HF_TOKEN")
        if not hf_token:
            print("Warning: HF_TOKEN environment variable not set. Make sure you are logged in via `huggingface-cli login` or have set HF_TOKEN for read access to the Hugging Face Dataset.")
        
        hf_fs = HfFileSystem(token=hf_token)

        if not Settings.embed_model:
            print("Error: Settings.embed_model not configured. Cannot load RAG index.")
            raise ValueError("Settings.embed_model is not set.")
        if not Settings.llm:
            print("Error: Settings.llm not configured. Cannot create RAG query engine.")
            raise ValueError("Settings.llm is not set.")

        print("Loading RAG index from Hugging Face storage...")
        storage_context = StorageContext.from_defaults(persist_dir=f"datasets/{HF_DATASET_ID}/{HF_VECTOR_STORE_SUBDIR}", fs=hf_fs)
        
        index = load_index_from_storage(storage_context, embed_model=Settings.embed_model)
        print(f"RAG index loaded successfully from Hugging Face Hub.")

        query_engine = index.as_query_engine(llm=Settings.llm)
        print("RAG query engine created.")

        def execute_rag_query(input: str):
                 """Executes a query against the RAG engine and returns the text response."""
                 try:
                     response_obj = query_engine.query(input)
                     text_response = str(response_obj.response)

                     sources_info_parts = []
                     citation_counter = 1
                     assigned_pdf_citations = {}

                     if response_obj.source_nodes:
                         for source_node in response_obj.source_nodes:
                             metadata = source_node.node.metadata
                             node_text_snippet = source_node.node.get_text()[:100]

                             if 'file_path' in metadata:
                                 file_path = metadata['file_path']
                                 file_name = os.path.basename(file_path)
                                 
                                 # Determine the correct Hugging Face path for the source file
                                 hf_relative_path_in_dataset = None
                                 if file_path.startswith(SOURCE_DATA_DIR):
                                     # Example: file_path = /project_root/ragdb/articles/file.pdf
                                     # We want: articles/file.pdf
                                     hf_relative_path_in_dataset = os.path.relpath(file_path, PROJECT_ROOT)
                                 elif file_path.startswith(WEB_MARKDOWN_PATH):
                                     # Example: file_path = /project_root/ragdb/web_markdown/file.md
                                     # We want: web_markdown/file.md
                                     hf_relative_path_in_dataset = os.path.relpath(file_path, PROJECT_ROOT)
                                 elif file_path.startswith(ADDITIONAL_SOURCE_DATA_DIR):
                                     # Example: file_path = /project_root/ragdb/source_data/file.pdf
                                     # We want: source_data/file.pdf (which maps to HF_ADDITIONAL_SOURCE_DATA_UPLOAD_PATH)
                                     # This needs to be relative to PROJECT_ROOT, then map to the HF upload path
                                     relative_to_additional_source = os.path.relpath(file_path, ADDITIONAL_SOURCE_DATA_DIR)
                                     hf_relative_path_in_dataset = os.path.join(HF_ADDITIONAL_SOURCE_DATA_UPLOAD_PATH, relative_to_additional_source)
                                 
                                 download_url = None
                                 if hf_relative_path_in_dataset:
                                     # URL-encode the entire relative path within the dataset, keeping slashes
                                     encoded_hf_relative_path = urllib.parse.quote(hf_relative_path_in_dataset, safe='/')
                                     download_url = f"https://huggingface.co/datasets/{HF_DATASET_ID}/resolve/main/{encoded_hf_relative_path}"
                                 else:
                                     # Fallback for local paths not mapped to HF dataset structure
                                     print(f"Warning: Could not map local file_path '{file_path}' to a known Hugging Face source subfolder. Providing local file:// URI.")
                                     # For local paths, ensure it's still a file:// URI and also encode it
                                     download_url = f"file://{urllib.parse.quote(file_path)}"

                                 # print(f"DEBUG: Constructed download_url: {download_url}") # Removed debug print
                                 
                                 current_citation_number = None
                                 if file_path not in assigned_pdf_citations:
                                     assigned_pdf_citations[file_path] = citation_counter
                                     current_citation_number = citation_counter
                                     citation_counter += 1
                                 else:
                                     current_citation_number = assigned_pdf_citations[file_path]
                                 
                                 source_data = {
                                     "type": "pdf", 
                                     "name": file_name, 
                                     "path": download_url, # Use the constructed download URL
                                     "snippet": node_text_snippet + "...",
                                     "citation_number": current_citation_number
                                 }
                                 sources_info_parts.append(f"---RAG_SOURCE---{json.dumps(source_data)}")
                             elif 'url' in metadata:
                                 url = metadata['url']
                                 title = metadata.get('title', url)
                                 source_data = {"type": "web", "url": url, "title": title, "snippet": node_text_snippet + "..."}
                                 sources_info_parts.append(f"---RAG_SOURCE---{json.dumps(source_data)}")

                     if sources_info_parts:
                         return text_response + "\n" + "\n".join(sources_info_parts)
                     else:
                         return text_response

                 except Exception as e:
                     print(f"Error during RAG query execution: {e}")
                     return f"Error querying the knowledge base: {e}"

        return FunctionTool.from_defaults(
                 fn=execute_rag_query,
                 name="rag_dissertation_retriever",
                 description=(
                    f"Retrieves relevant information from the dissertation knowledge base (persisted on Hugging Face at '{HF_DATASET_ID}/{HF_VECTOR_STORE_SUBDIR}'). "
                    f"Source documents are available in subfolders like '{HF_ADDITIONAL_SOURCE_DATA_UPLOAD_PATH}', 'articles', and 'web_markdown' within the '{HF_DATASET_ID}' dataset. "
                    "Use this for specific institutional knowledge or previously saved research. "
                    "The tool's output will include the textual answer and may be followed by structured references "
                    "(e.g., to PDF files or web URLs) using '---RAG_SOURCE---' markers. "
                    "For PDF sources, the structured reference will include a 'citation_number'. "
                    "When you use information from a PDF source in your response, you MUST append its citation number "
                    "in brackets (e.g., '[1]', '[2]') to the relevant sentence or claim. "
                    "The query to the knowledge base should be provided as the 'input' string argument."
                 ),
             )

    except Exception as e:
        error_message = f"Error initializing or loading RAG tool from Hugging Face ({HF_DATASET_ID}/{HF_VECTOR_STORE_SUBDIR}): {e}"
        print(error_message)
        return FunctionTool.from_defaults(
            fn=lambda *args, msg=error_message, **kwargs: msg,
            name="rag_dissertation_retriever",
            description=f"The dissertation knowledge base (from Hugging Face: {HF_DATASET_ID}/{HF_VECTOR_STORE_SUBDIR}) is currently unavailable due to an error during initialization."
        )


# --- Tool Collections for Specialized Agents ---

def get_search_tools(max_results: int = 5):
    """Initializes and returns a list of search-related tools."""
    tools = []
    # Pass max_results to the tool's query method if needed, not the constructor
    ddg_tool = get_duckduckgo_tool() 
    tavily_tool = get_tavily_tool()
    wiki_tool = get_wikipedia_tool()

    if ddg_tool: tools.append(ddg_tool)
    if tavily_tool: tools.append(tavily_tool)
    if wiki_tool: tools.append(wiki_tool)

    print(f"Initialized {len(tools)} search tools.")
    return tools

# --- Code Interpreter Tool Setup ---
def get_coder_tools():
    """
    Initialize and return code interpreter tools for a LlamaIndex agent.
    
    Returns:
        list: A list of tools from the CodeInterpreterToolSpec
    """
    # Reverting to FunctionTool with safe_code_interpreter
    try:
        coder_description = (
            "Executes Python code. Input is the Python code string. "
            "If the code executes successfully, the output will be a string containing 'Executed Code:', 'StdOut:', and 'StdErr:' (which will be empty on success). "
            "If the code has syntax or runtime errors, the output will start with 'Python Code Execution Error:' and include 'Executed Code:', 'Error Output (StdErr):', and any 'Standard Output (StdOut):'. "
            "Use this for calculations, data manipulation, plotting, or other tasks requiring code execution. "
            f"If the code needs to generate and save files for the user to download, it MUST save them into the '{UI_ACCESSIBLE_WORKSPACE}' directory. "
            "After saving a file, the code MUST print a line to stdout exactly in the format '---DOWNLOAD_FILE---filename.ext' (e.g., '---DOWNLOAD_FILE---plot.png'). "
            "The code can also print the full, absolute path of any saved file to stdout if it's useful for context, in addition to the download marker. "
            "You should report any errors from 'Error Output (StdErr):' back to the user."
        )
        code_interpreter_tool = FunctionTool.from_defaults(
            fn=safe_code_interpreter,
            name="code_interpreter",
            description=coder_description
        )
        if code_interpreter_tool is None:
            print("CRITICAL_TOOLS: FunctionTool.from_defaults returned None for code_interpreter!")
            return []
        print(f"Code interpreter tool created.")
        return [code_interpreter_tool]
    except Exception as e:
        print(f"CRITICAL_TOOLS: Exception during FunctionTool creation for code_interpreter: {e}")
        import traceback
        traceback.print_exc()
        return [] # Return empty list to ensure it's falsy
